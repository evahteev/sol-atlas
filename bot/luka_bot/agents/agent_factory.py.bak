from pydantic_ai.agent import Agent
from pydantic_ai.settings import ModelSettings
from pydantic_ai.models.openai import OpenAIModel
from aiogram.utils.i18n import gettext as _
from loguru import logger
from typing import List, Any

from .context import ConversationContext
# Phase 4-5: Import available tools
from .tools import support_tools
from .tools import youtube_tools
from .tools import knowledge_base_tools  # Phase 5: KB search
# Phase 4+: from .tools import workflow_tools, camunda_tools
from luka_bot.core.config import settings
from luka_bot.services.user_profile_service import UserProfileService

async def get_language_instruction_for_user(user_id: int) -> str:
    """Get language instruction based on user's language preference."""
    try:
        # Get user language from profile service
        profile = await UserProfileService.get_user_profile(user_id)
        language_code = profile.language if profile else "en"
        logger.info(f"ðŸŒ User {user_id} language preference: {language_code}")
        
        if language_code == 'ru':
            return "\n\n**IMPORTANT LANGUAGE INSTRUCTION**: Always respond in Russian language (Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº). Use Russian for all your responses unless the user explicitly requests another language. Note: This instruction applies only to your response language, not to content retrieval (e.g., YouTube captions should still prefer English by default)."
        elif language_code == 'en':
            return "\n\n**IMPORTANT LANGUAGE INSTRUCTION**: Always respond in English language. Use English for all your responses unless the user explicitly requests another language. Note: This instruction applies only to your response language, not to content retrieval."
        else:
            # Default to Russian for unknown language codes
            logger.info(f"ðŸŒ Unknown language code '{language_code}' for user {user_id}, defaulting to Russian")
            return "\n\n**IMPORTANT LANGUAGE INSTRUCTION**: Always respond in Russian language (Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº). Use Russian for all your responses unless the user explicitly requests another language. Note: This instruction applies only to your response language, not to content retrieval (e.g., YouTube captions should still prefer English by default)."
    except Exception as e:
        logger.warning(f"ðŸŒ Error getting language for user {user_id}: {e}, defaulting to Russian")
        return "\n\n**IMPORTANT LANGUAGE INSTRUCTION**: Always respond in Russian language (Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº). Use Russian for all your responses unless the user explicitly requests another language. Note: This instruction applies only to your response language, not to content retrieval (e.g., YouTube captions should still prefer English by default)."


def build_dynamic_system_prompt(tool_modules: List[Any], num_dynamic_tasks: int = 0, language_instruction: str = "") -> str:
    """Build system prompt dynamically based on available tool modules."""
    
    # Get localized base prompt (with fallback for testing)
    try:
        base_prompt = _("agent_base_prompt")
        # Check if we got a translation key back instead of actual text
        if base_prompt == "agent_base_prompt" or len(base_prompt) < 50:
            raise LookupError("Translation key not resolved")
    except (LookupError, Exception):
        # Fallback for test environment where i18n context is not set or localization failed
        application_name = settings.LUKA_NAME or "Luka"
        base_prompt = f"""You are {application_name}, an intelligent AI assistant integrated into Telegram.
You're a conversational AI that helps users with a wide range of tasks - from answering questions and having discussions, to organizing information and managing their workflow.

Your core capabilities include:
- Having natural, helpful conversations on any topic
- Searching through users' personal and group knowledge bases to find relevant information
- Analyzing YouTube videos by extracting and discussing their transcripts
- Helping organize thoughts and information across multiple conversation threads
- Assisting with task management and productivity
- Providing information, explanations, and creative help

**When to use tools:**
- When users ask about information from their message history or knowledge base, use the search_knowledge_base tool
- When users share YouTube links or ask about video content, use the get_youtube_transcript tool
- Otherwise, engage naturally using your general knowledge and conversational abilities

## AVAILABLE BOT COMMANDS AND FEATURES

The bot has the following commands that users can access:

**Core Commands:**
- `/start` - Main entry point showing Quick Actions menu with:
  â€¢ Chats (count) - Access conversation threads
  â€¢ Tasks (count) - View GTD-organized tasks
  â€¢ Profile - User settings and preferences

- `/chat` - Conversation thread management
  â€¢ View all existing conversation threads
  â€¢ Create new threads with custom names
  â€¢ Switch between different conversation contexts
  â€¢ Each thread maintains separate conversation history
  â€¢ Threads can have custom system prompts and KB connections

- `/search` - Knowledge Base search functionality
  â€¢ Create/access dedicated "chatbot_search" thread
  â€¢ Select which KBs to search from:
    - Personal KB (user's own indexed content)
    - Group KBs (from Telegram groups bot is added to)
  â€¢ Toggle multiple KBs simultaneously
  â€¢ Search across selected knowledge bases

- `/tasks` - GTD (Getting Things Done) task management (Coming Soon)
  â€¢ Inbox - New unprocessed tasks
  â€¢ Next - Tasks ready to start
  â€¢ Waiting - Tasks blocked/waiting on others
  â€¢ Scheduled - Future dated tasks
  â€¢ Someday - Future ideas and backlog
  â€¢ Integration with Camunda workflows planned
  â€¢ LLM-powered task control commands

- `/groups` - Group management features (Coming Soon)
  â€¢ Add bot to Telegram groups
  â€¢ Map groups/topics to dedicated threads
  â€¢ Set group owners for thread management
  â€¢ Import recent group history into threads
  â€¢ Auto-switch to group thread on notifications

- `/profile` - User profile and settings
  â€¢ View user information and statistics
  â€¢ Change interface language (English/Russian)
  â€¢ Bot preferences (system prompt, KB settings, default model)
  â€¢ View running processes (Camunda integration)
  â€¢ Access usage statistics and leaderboard

- `/reset` - Clear all user data
  â€¢ Delete all conversation threads
  â€¢ Clear conversation history
  â€¢ Reset session state
  â€¢ Fresh start for the user

**Key Features:**
1. **Multi-threaded Conversations** - Users can maintain separate conversation contexts with different configurations
2. **Knowledge Base Integration** - Search across personal and group knowledge bases
3. **YouTube Integration** - Extract and analyze YouTube video transcripts
4. **Language Support** - Interface available in English and Russian
5. **Thread Customization** - Each thread can have custom system prompts and KB connections
6. **Task Management** - GTD-style organization with Camunda workflow integration (in development)
7. **Group Integration** - Connect Telegram groups to dedicated conversation threads (in development)

**Your Role as Assistant:**
- Help users navigate and use these features effectively
- Guide them to the appropriate commands for their needs
- Explain feature capabilities when asked
- Use available tools (search_knowledge_base, get_youtube_transcript, etc.) to fulfill user requests
- Be proactive in suggesting relevant features based on user needs
- Inform users about "Coming Soon" features when they ask about unavailable functionality

Always be helpful, professional, and guide users toward completing their available tasks."""
    
    # Add tool-specific sections
    tool_sections = []
    for module in tool_modules:
        if hasattr(module, 'get_prompt_description'):
            try:
                description = module.get_prompt_description()
                # Check if we got a translation key back instead of actual text
                if description and len(description) > 10 and not description.endswith("_prompt"):
                    tool_sections.append(description)
                else:
                    logger.warning(f"Module {module.__name__} returned invalid prompt description: {description}")
            except Exception as e:
                logger.warning(f"Error getting prompt description from module {module.__name__}: {e}")
    
    # Add dynamic task info if available
    if num_dynamic_tasks > 0:
        try:
            dynamic_section = _("dynamic_tasks_prompt").format(num_tasks=num_dynamic_tasks)
            # Check if we got a translation key back instead of actual text
            if dynamic_section == "dynamic_tasks_prompt" or len(dynamic_section) < 20:
                raise LookupError("Translation key not resolved")
        except (LookupError, Exception):
            # Fallback for test environment where i18n context is not set or localization failed
            dynamic_section = f"\n\nYou have {num_dynamic_tasks} dynamic task(s) available to execute for this user. Use the appropriate task execution tools when the user requests task-related actions."
        tool_sections.append(dynamic_section)
    
    # Combine all sections with language instruction
    sections_to_combine = [base_prompt + language_instruction]
    if tool_sections:
        sections_to_combine.extend(tool_sections)
    
    full_prompt = "\n\n".join(sections_to_combine)
    
    return full_prompt

async def create_static_agent_with_basic_tools(user_id: int) -> Agent:
    """Create a fast agent with only static tools (no dynamic task tools)."""
    logger.info("Creating static agent with basic tools for immediate response")
    
    # Create model with automatic provider fallback (Ollama â†’ OpenAI)
    from luka_bot.services.llm_model_factory import create_llm_model_with_fallback
    
    model = await create_llm_model_with_fallback(
        context=f"user_{user_id}",
        model_settings=ModelSettings(
            temperature=settings.LLM_TEMPERATURE,
            top_p=settings.LLM_TOP_P,
            frequency_penalty=settings.LLM_FREQUENCY_PENALTY,
            presence_penalty=settings.LLM_PRESENCE_PENALTY,
            stop_sequences=["\n\n\n", "User:", "Assistant:"],
            max_tokens=settings.LLM_MAX_TOKENS,
            timeout=settings.OLLAMA_TIMEOUT
        )
    )
    
    # Build system prompt with user's language preference
    language_instruction = await get_language_instruction_for_user(user_id)
    
    # Phase 4-5: Build system prompt; include youtube & KB in description
    tool_modules = [support_tools, youtube_tools, knowledge_base_tools]
    system_prompt = build_dynamic_system_prompt(tool_modules, 0, language_instruction)
    
    # Phase 4-5: Static tools available
    # Important: YouTube tool is invoked heuristically in LLM service to avoid duplicate agent invocations
    # FIX 32b: Revert to Tool() wrappers - issue was the LLM passing string "conversation"
    static_tools = [
        *support_tools.get_tools(),
        *knowledge_base_tools.get_tools(),  # Phase 5: KB search tool
        # *youtube_tools.get_tools(),  # handled via heuristic path
        # Phase 4+: *workflow_tools.get_tools()
    ]
    
    logger.info(f"Static agent will have {len(static_tools)} tools")
    for tool in static_tools:
        tool_name = getattr(tool, 'name', str(tool))
        logger.info(f"  - {tool_name}")
    
    # Create agent WITH tools
    try:
        agent = Agent(
            model=model,  # Uses fallback-enabled model
            deps_type=ConversationContext,
            system_prompt=system_prompt,
            tools=static_tools,  # Pass Tool objects
            end_strategy='exhaustive'  # Allow both text and tool execution
        )
        logger.info("Agent created successfully with tools")
        
        # Verify tools are attached
        logger.debug(f"Agent has 'tools' attribute: {hasattr(agent, 'tools')}")
        if hasattr(agent, 'tools'):
            logger.debug(f"Agent.tools: {agent.tools}")
        logger.debug(f"Agent has '_function_toolset': {hasattr(agent, '_function_toolset')}")
        if hasattr(agent, '_function_toolset'):
            logger.debug(f"_function_toolset: {agent._function_toolset}")
            logger.debug(f"_function_toolset attributes: {[attr for attr in dir(agent._function_toolset) if not attr.startswith('__')]}")
            
            # Try all possible tool storage locations
            for attr in ['_tools', 'tools', '_tool_defs', 'tool_defs', '_definitions']:
                if hasattr(agent._function_toolset, attr):
                    value = getattr(agent._function_toolset, attr)
                    if isinstance(value, dict):
                        logger.debug(f"  Keys: {list(value.keys())}")
        
        # Check other agent attributes
        logger.debug(f"Agent toolsets attribute: {getattr(agent, 'toolsets', 'not found')}")
        logger.debug(f"Agent _user_toolsets: {getattr(agent, '_user_toolsets', 'not found')}")
    except Exception as e:
        logger.warning(f"Agent creation with tools failed: {e}")
        # Fallback: create agent without tools
        agent = Agent(
            model=ollama_model,
            deps_type=ConversationContext,
            system_prompt=system_prompt,
            end_strategy='exhaustive'  # Allow both text and tool execution
        )
        logger.info("Created fallback agent without tools")
    
    logger.info("Static agent created successfully")
    return agent

async def create_agent_with_user_tasks(ctx: ConversationContext) -> Agent:
    """Create an agent instance with user-specific dynamic task tools."""
    
    logger.info(f"Starting create_agent_with_user_tasks for user {ctx.user_id}")
    logger.debug(f"Context received - user_uuid: {ctx.user_uuid} (type: {type(ctx.user_uuid)})")
    logger.debug(f"Context received - camunda_user_id: {ctx.camunda_user_id}")
    logger.debug(f"Context received - camunda_key: {ctx.camunda_key}")
    
    try:
        # Get dynamic task tools for this user
        # Phase 5: Camunda dynamic tools
        # logger.info("Getting dynamic task tools...")
        # dynamic_tools = await camunda_tools.get_dynamic_tools_for_user(ctx)
        # logger.info(f"Got {len(dynamic_tools)} dynamic tools")
        dynamic_tools = []  # Phase 4: No Camunda integration yet
        
        # Create model with automatic provider fallback (Ollama â†’ OpenAI)
        logger.info("Configuring LLM model with automatic fallback...")
        
        from luka_bot.services.llm_model_factory import create_llm_model_with_fallback
        
        model = await create_llm_model_with_fallback(
            context=f"user_{user_id}_dynamic",
            model_settings=ModelSettings(
                temperature=settings.LLM_TEMPERATURE,
                top_p=settings.LLM_TOP_P,
                frequency_penalty=settings.LLM_FREQUENCY_PENALTY,  # Penalize repetition
                presence_penalty=settings.LLM_PRESENCE_PENALTY,   # Encourage variety
                stop_sequences=["\n\n\n", "User:", "Assistant:"],  # Stop sequences
                max_tokens=settings.LLM_MAX_TOKENS,
                timeout=settings.OLLAMA_TIMEOUT
            )
        )
        logger.info(f"LLM model configured with fallback: {model.model_name}")
        
        # DEBUG logging removed - model now uses automatic fallback system
            
            # Try to intercept requests
            if hasattr(client, 'post'):
                original_post = client.post
                async def logged_post(*args, **kwargs):
                    logger.error("ðŸ”§ HTTP DEBUG: =================== POST REQUEST START ===================")
                    logger.error("ðŸ”§ HTTP DEBUG: POST request to model")
                    logger.error(f"ðŸ”§ HTTP DEBUG: args: {args}")
                    logger.error(f"ðŸ”§ HTTP DEBUG: URL: {args[0] if args else 'unknown'}")
                    logger.error(f"ðŸ”§ HTTP DEBUG: All kwargs keys: {list(kwargs.keys())}")
                    logger.error(f"ðŸ”§ HTTP DEBUG: Provider base_url: {getattr(ollama_provider, 'base_url', 'unknown')}")
                    logger.error("ðŸ”§ HTTP DEBUG: =================== POST REQUEST END ===================")
                    
                    # Extract JSON data from body parameter (OpenAI uses 'body' not 'json')
                    json_data = None
                    if 'body' in kwargs:
                        body = kwargs['body']
                        logger.info(f"ðŸ”§ HTTP DEBUG: Body type: {type(body)}")
                        if hasattr(body, 'decode'):
                            try:
                                import json as json_lib
                                json_data = json_lib.loads(body.decode('utf-8'))
                                logger.info("ðŸ”§ HTTP DEBUG: Successfully parsed JSON from body")
                            except Exception as e:
                                logger.info(f"ðŸ”§ HTTP DEBUG: Failed to parse JSON from body: {e}")
                        elif isinstance(body, dict):
                            json_data = body
                            logger.info("ðŸ”§ HTTP DEBUG: Body is already dict")
                        else:
                            logger.info(f"ðŸ”§ HTTP DEBUG: Body content (first 200 chars): {str(body)[:200]}")
                    elif 'json' in kwargs:
                        json_data = kwargs['json']
                        logger.info("ðŸ”§ HTTP DEBUG: Using json parameter")
                    
                    if json_data:
                        logger.info(f"ðŸ”§ HTTP DEBUG: Request payload keys: {list(json_data.keys()) if isinstance(json_data, dict) else 'not dict'}")
                        if isinstance(json_data, dict):
                            logger.info(f"ðŸ”§ HTTP DEBUG: Model: {json_data.get('model', 'not specified')}")
                            logger.info(f"ðŸ”§ HTTP DEBUG: Has tools: {'tools' in json_data}")
                            logger.info(f"ðŸ”§ HTTP DEBUG: Has functions: {'functions' in json_data}")
                            if 'tools' in json_data:
                                tools = json_data['tools']
                                logger.info(f"ðŸ”§ HTTP DEBUG: Tools count: {len(tools) if isinstance(tools, list) else 'not list'}")
                                if isinstance(tools, list) and tools:
                                    for i, tool in enumerate(tools[:3]):  # Log first 3 tools
                                        if isinstance(tool, dict) and 'function' in tool:
                                            func_name = tool['function'].get('name', 'unnamed')
                                            logger.info(f"ðŸ”§ HTTP DEBUG: Tool {i+1}: {func_name}")
                                        else:
                                            logger.info(f"ðŸ”§ HTTP DEBUG: Tool {i+1}: {tool}")
                            if 'messages' in json_data:
                                messages = json_data['messages']
                                logger.info(f"ðŸ”§ HTTP DEBUG: Messages count: {len(messages) if isinstance(messages, list) else 'not list'}")
                                if isinstance(messages, list):
                                    for i, msg in enumerate(messages):
                                        if isinstance(msg, dict):
                                            role = msg.get('role', 'unknown')
                                            content = msg.get('content', '')
                                            logger.info(f"ðŸ”§ HTTP DEBUG: Message {i+1}: role={role}, length={len(content)}")
                                            if role == 'system':
                                                logger.info(f"ðŸ”§ HTTP DEBUG: System message content (first 200 chars): {content[:200]}")
                                                logger.info(f"ðŸ”§ HTTP DEBUG: System message has 'search_knowledge_base': {'search_knowledge_base' in content}")
                                                logger.info(f"ðŸ”§ HTTP DEBUG: System message has 'tool': {'tool' in content.lower()}")
                    
                    try:
                        logger.info("ðŸ”§ HTTP DEBUG: Making actual request...")
                        response = await original_post(*args, **kwargs)
                        logger.info(f"ðŸ”§ HTTP DEBUG: Request successful, response type: {type(response)}")
                        return response
                    except Exception as e:
                        logger.error(f"ðŸ”§ HTTP DEBUG: Request failed with error: {e}")
                        logger.error(f"ðŸ”§ HTTP DEBUG: Error type: {type(e)}")
                        if hasattr(e, 'response'):
                            logger.error(f"ðŸ”§ HTTP DEBUG: Response status: {getattr(e.response, 'status_code', 'unknown')}")
                            logger.error(f"ðŸ”§ HTTP DEBUG: Response text: {getattr(e.response, 'text', 'unknown')}")
                        raise
                client.post = logged_post
                logger.info("ðŸ”§ HTTP DEBUG: Added request logging to provider client")
        elif hasattr(ollama_model, '_client') and ollama_model._client:
            client = ollama_model._client  
            logger.info(f"ðŸ”§ HTTP DEBUG: Model client type: {type(client)}")
        else:
            logger.info("ðŸ”§ HTTP DEBUG: Could not access client for request logging")
        
        # Collect MINIMAL tools (only dynamic + knowledge base for testing)
        logger.info("ðŸ¤– AGENT DEBUG: Collecting MINIMAL tools for function calling test...")
        
        # TEMPORARILY DISABLE: basic_camunda_tools = camunda_tools.get_tools()
        # TEMPORARILY DISABLE: support_tools_list = support_tools.get_tools()
        kb_tools_list = knowledge_base_tools.get_tools()
        youtube_tools_list = youtube_tools.get_tools()
        workflow_tools_list = workflow_tools.get_tools()
        
        # logger.info(f"ðŸ¤– AGENT DEBUG: Basic camunda tools: {len(basic_camunda_tools)}")
        # logger.info(f"ðŸ¤– AGENT DEBUG: Support tools: {len(support_tools_list)}")
        logger.info(f"ðŸ¤– AGENT DEBUG: Knowledge base tools: {len(kb_tools_list)}")
        logger.info(f"ðŸ¤– AGENT DEBUG: YouTube tools: {len(youtube_tools_list)}")
        logger.info(f"ðŸ¤– AGENT DEBUG: Workflow tools: {len(workflow_tools_list)}")
        logger.info(f"ðŸ¤– AGENT DEBUG: Dynamic task tools: {len(dynamic_tools)}")
        
        # MINIMAL TOOL SET: Only dynamic tasks + knowledge base + YouTube + workflow
        all_tools = [
            *kb_tools_list,
            *youtube_tools_list,
            *workflow_tools_list,
            *dynamic_tools
        ]
        logger.info(f"ðŸ¤– AGENT DEBUG: Total tools collected: {len(all_tools)}")
        
        # Log tool details for debugging
        logger.info("ðŸ¤– AGENT DEBUG: MINIMAL TOOL ANALYSIS:")
        # logger.info(f"ðŸ¤– AGENT DEBUG:   - Basic Camunda tools: {[getattr(tool, 'name', str(tool)) for tool in basic_camunda_tools]}")
        # logger.info(f"ðŸ¤– AGENT DEBUG:   - Support tools: {[getattr(tool, 'name', str(tool)) for tool in support_tools_list]}")
        logger.info(f"ðŸ¤– AGENT DEBUG:   - Knowledge base tools: {[getattr(tool, 'name', str(tool)) for tool in kb_tools_list]}")
        logger.info(f"ðŸ¤– AGENT DEBUG:   - YouTube tools: {[getattr(tool, 'name', str(tool)) for tool in youtube_tools_list]}")
        logger.info(f"ðŸ¤– AGENT DEBUG:   - Workflow tools: {[getattr(tool, 'name', str(tool)) for tool in workflow_tools_list]}")
        logger.info(f"ðŸ¤– AGENT DEBUG:   - Dynamic task tools: {[getattr(tool, 'name', str(tool)) for tool in dynamic_tools]}")
        
        # Log tool descriptions to understand what each tool does
        logger.info("ðŸ¤– AGENT DEBUG: DETAILED TOOL DESCRIPTIONS:")
        for i, tool in enumerate(all_tools):
            tool_name = getattr(tool, 'name', 'unknown')
            tool_desc = getattr(tool, 'description', 'no description')
            logger.info(f"ðŸ¤– AGENT DEBUG:   Tool {i+1}: name='{tool_name}', description='{tool_desc}'")
        
        # Build SIMPLIFIED system prompt for minimal tools
        logger.info("Building SIMPLIFIED system prompt for minimal tools...")
        
        # Get user's language preference
        language_instruction = await get_language_instruction_for_user(ctx.user_id)
        
        # Create a simple, focused system prompt for function calling test
        application_name = settings.LUKA_NAME or "Luka"
        system_prompt = f"""You are {application_name}, an intelligent AI assistant.

You can help users with:
- Service information and support questions
- Executing available business process tasks

**IMPORTANT**: When users ask about services, ALWAYS use the search_knowledge_base tool first to provide accurate information from our knowledge base.

You have {len(dynamic_tools)} task(s) available to execute for this user. Use the appropriate task execution tools when the user requests task-related actions.

Always be helpful and professional.{language_instruction}"""
        
        logger.info(f"Simplified system prompt built - length: {len(system_prompt)} characters")
        
        logger.info(f"Creating pydantic-ai Agent with {len(dynamic_tools)} dynamic task tools for user {ctx.user_id}")
        
        try:
            # Create agent with tools
            agent_with_tasks = Agent(
                model=ollama_model,
                deps_type=ConversationContext,
                system_prompt=system_prompt,
                tools=all_tools,  # Pass all tools directly
                end_strategy='exhaustive'  # Allow both text and tool execution
            )
            
            logger.info(f"Agent created successfully with {len(all_tools)} tools")
            
            # DEBUG: Verify agent has tools properly attached
            logger.info("ðŸ”§ AGENT TOOLS DEBUG: Verifying agent tool attachment...")
            logger.info(f"ðŸ”§ AGENT TOOLS DEBUG: Agent has 'tools' attribute: {hasattr(agent_with_tasks, 'tools')}")
            logger.info(f"ðŸ”§ AGENT TOOLS DEBUG: Agent has '_function_toolset' attribute: {hasattr(agent_with_tasks, '_function_toolset')}")
            
            if hasattr(agent_with_tasks, '_function_toolset'):
                toolset = agent_with_tasks._function_toolset
                logger.info(f"ðŸ”§ AGENT TOOLS DEBUG: Function toolset type: {type(toolset)}")
                logger.info(f"ðŸ”§ AGENT TOOLS DEBUG: Function toolset attributes: {[attr for attr in dir(toolset) if not attr.startswith('_')]}")
                
                # Check for various tool storage locations
                for attr_name in ['tools', '_tools', 'tool_defs', '_tool_defs', 'definitions', '_definitions']:
                    if hasattr(toolset, attr_name):
                        attr_value = getattr(toolset, attr_name)
                        logger.info(f"ðŸ”§ AGENT TOOLS DEBUG: toolset.{attr_name}: {type(attr_value)} - {list(attr_value.keys()) if isinstance(attr_value, dict) else len(attr_value) if hasattr(attr_value, '__len__') else attr_value}")
            
            # DEBUG: Check system prompt contains tool instructions
            logger.info("ðŸ”§ SYSTEM PROMPT DEBUG: Verifying system prompt...")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: System prompt length: {len(system_prompt)} characters")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: Contains 'tool': {'tool' in system_prompt.lower()}")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: Contains 'function': {'function' in system_prompt.lower()}")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: Contains 'search_knowledge_base': {'search_knowledge_base' in system_prompt}")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: First 500 chars: {system_prompt[:500]}")
            logger.info(f"ðŸ”§ SYSTEM PROMPT DEBUG: Last 500 chars: {system_prompt[-500:]}")
            
            return agent_with_tasks
            
        except Exception as e:
            # If model doesn't support tools, create a basic agent without tools
            logger.error(f"ðŸš¨ AGENT CREATION ERROR: Model {settings.OLLAMA_MODEL_NAME} does not support function calling/tools!")
            logger.error(f"ðŸš¨ AGENT CREATION ERROR: Original exception: {e}")
            logger.error(f"ðŸš¨ AGENT CREATION ERROR: Exception type: {type(e)}")
            import traceback
            logger.error(f"ðŸš¨ AGENT CREATION ERROR: Full traceback: {traceback.format_exc()}")
            logger.warning("Creating fallback agent without tools...")
            
            # Create simpler system prompt without tool references
            try:
                fallback_prompt = _("agent_fallback_prompt")
            except LookupError:
                application_name = settings.LUKA_NAME or "Luka"
                fallback_prompt = f"You are {application_name}, a helpful AI assistant. Answer user questions as best you can."
            
            # Add user's language instruction to fallback prompt
            language_instruction = await get_language_instruction_for_user(ctx.user_id)
            fallback_prompt += language_instruction

            agent_without_tools = Agent(
                model=ollama_model,
                deps_type=ConversationContext,
                system_prompt=fallback_prompt,
                end_strategy='exhaustive'  # Allow both text and tool execution
            )
            
            logger.warning("ðŸš¨ FALLBACK: Created agent WITHOUT TOOLS - this explains why no tool calls are happening!")
            return agent_without_tools
        
    except Exception as e:
        logger.error(f"Error in create_agent_with_user_tasks: {e}")
        logger.error(f"Error type: {type(e)}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise

async def create_simple_agent_without_tools(user_id: int) -> Agent:
    """Create a simple agent without tools for models that don't support function calling."""
    logger.info("Creating simple agent without tools")
    
    # Configure Ollama model
    logger.debug(f"Using Ollama URL: {settings.OLLAMA_URL}")
    logger.debug(f"Using Ollama model: {settings.OLLAMA_MODEL_NAME}")
    
    from pydantic_ai.providers.ollama import OllamaProvider
    
    # Use Ollama URL directly (Ollama doesn't use /v1 endpoint)
    ollama_url = settings.OLLAMA_URL
    
    # Create custom Ollama provider with our base URL
    ollama_provider = OllamaProvider(base_url=ollama_url)
    
    # Create model settings to reduce repetition
    model_settings = ModelSettings(
        temperature=0.7,
        top_p=0.9,
        frequency_penalty=0.5,  # Penalize repetition
        presence_penalty=0.3,   # Encourage variety
        stop_sequences=["\n\n\n", "User:", "Assistant:"],  # Stop sequences
        max_tokens=2000,
        timeout=30.0
    )
    
    ollama_model = OpenAIModel(
        model_name=settings.OLLAMA_MODEL_NAME,
        provider=ollama_provider,
        settings=model_settings
    )
    
    # Simple system prompt without tool references
    simple_prompt = _("agent_simple_prompt")
    
    # Add user's language instruction to simple prompt
    language_instruction = await get_language_instruction_for_user(user_id)
    simple_prompt += language_instruction

    simple_agent = Agent(
        model=ollama_model,
        deps_type=ConversationContext,
        system_prompt=simple_prompt,
        end_strategy='exhaustive'  # Allow both text and tool execution
    )
    
    logger.info("Simple agent created successfully without tools")
    return simple_agent